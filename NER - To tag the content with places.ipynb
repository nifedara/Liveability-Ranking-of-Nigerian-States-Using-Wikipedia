{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8afa6b03",
   "metadata": {},
   "source": [
    "---This notebook is supposed to be interactive, but I did this project in chunks and in different notebooks and running it again is time consuming as the dataset is large. So I will just leave the code that I used to achieve everything I did here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a265b587",
   "metadata": {},
   "source": [
    "Getting the location in the title column of all the rows. So I can know what exact location the article is about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b21978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8154cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded here is the csv file that contains articles that mention Nigeria\n",
    "#a total of 55474 rows and 3 columns: 'id', 'title', 'text'\n",
    "\n",
    "wikidump = pd.read_csv(r'Desktop\\mergedDump.csv')\n",
    "\n",
    "\n",
    "#using spacy to get named entities in the title column\n",
    "#the found locations are set in a new column 'location'\n",
    "\n",
    "wikidump['text'] = wikidump['text'].astype(str) #cast type so the below function doesn't encounter any error. as spacy works\n",
    "                                                #on only string or Doc\n",
    "\n",
    "def get_loc(text):\n",
    "    doc = nlp(text)\n",
    "    results = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847af209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the function to get the location on the title column\n",
    "wikidump[\"location\"] = wikidump['title'].apply(get_loc)\n",
    "\n",
    "wikidump[\"location\"] = wikidump[\"location\"].apply(eval)  #this removes the quotation marks on the items in the location list\n",
    "\n",
    "wikidump['length'] = wikidump.location.apply(len) #to get the number of locations in each row of the new location column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243f6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove rows where the location is more or less than 1. so as to get rows(articles) that specially describe just one place\n",
    "\n",
    "wikidump = wikidump.loc[wikidump['length'] == 1] \n",
    "wikidump\n",
    "\n",
    "del wikidump['length'] #delete the length column. It is no longer needed as the filtering has now been done\n",
    "\n",
    "wikidump.reset_index(drop=True)\n",
    "\n",
    "#there are now 8444 rows × 4 columns left (id, title, text, location)\n",
    "\n",
    "\n",
    "#Data Cleaning\n",
    "wikidump = wikidump[wikidump[\"title\"].str.contains(\"Category:\") == False] #cleaning some irrelevant rows\n",
    "wikidump.reset_index(drop=True)\n",
    "\n",
    "#there are now 5081 rows × 4 columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
